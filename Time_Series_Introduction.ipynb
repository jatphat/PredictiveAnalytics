{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis in Python – A Comprehensive Guide with Examples\n",
    "\n",
    "https://www.machinelearningplus.com/time-series/time-series-analysis-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is a Time Series?\n",
    "\n",
    "Time series is a sequence of observations recorded at regular time intervals.\n",
    "\n",
    "Depending on the frequency of observations, a time series may typically be hourly, daily, weekly, monthly, quarterly and annual. \n",
    "\n",
    "Sometimes, you might have seconds and minute-wise time series as well, like, number of clicks and user visits every minute etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. How to import time series in python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.rcParams.update({'figure.figsize': (10, 7), 'figure.dpi': 120})\n",
    "\n",
    "# Import as Dataframe\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "ser.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What is panel data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset source: https://github.com/rouseguy\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/MarketArrivals.csv')\n",
    "df = df.loc[df.market=='MUMBAI', :]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizing a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data source: fpp pacakge in R.\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Draw Plot\n",
    "def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "plot_df(df, x=df.index, y=df.value, title='Monthly anti-diabetic drug sales in Australia from 1992 to 2008.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('datasets/AirPassengers.csv', parse_dates=['date'])\n",
    "x = df['date'].values\n",
    "y1 = df['value'].values\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16,5), dpi= 120)\n",
    "plt.fill_between(x, y1=y1, y2=-y1, alpha=0.5, linewidth=2, color='seagreen')\n",
    "plt.ylim(-800, 800)\n",
    "plt.title('Air Passengers (Two Side View)', fontsize=16)\n",
    "plt.hlines(y=0, xmin=np.min(df.date), xmax=np.max(df.date), linewidth=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "df['year'] = [d.year for d in df.date]\n",
    "df['month'] = [d.strftime('%b') for d in df.date]\n",
    "years = df['year'].unique()\n",
    "\n",
    "# Prep Colors\n",
    "np.random.seed(100)\n",
    "mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False)\n",
    "\n",
    "# Draw Plot\n",
    "plt.figure(figsize=(16,12), dpi= 80)\n",
    "for i, y in enumerate(years):\n",
    "    if i > 0:        \n",
    "        plt.plot('month', 'value', data=df.loc[df.year==y, :], color=mycolors[i], label=y)\n",
    "        plt.text(df.loc[df.year==y, :].shape[0]-.9, df.loc[df.year==y, 'value'][-1:].values[0], y, fontsize=12, color=mycolors[i])\n",
    "\n",
    "# Decoration\n",
    "plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='$Drug Sales$', xlabel='$Month$')\n",
    "plt.yticks(fontsize=12, alpha=.7)\n",
    "plt.title(\"Seasonal Plot of Drug Sales Time Series\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "df['year'] = [d.year for d in df.date]\n",
    "df['month'] = [d.strftime('%b') for d in df.date]\n",
    "years = df['year'].unique()\n",
    "\n",
    "# Draw Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)\n",
    "sns.boxplot(x='year', y='value', data=df, ax=axes[0])\n",
    "sns.boxplot(x='month', y='value', data=df.loc[~df.year.isin([1991, 2008]), :])\n",
    "\n",
    "# Set Title\n",
    "axes[0].set_title('Year-wise Box Plot\\n(The Trend)', fontsize=18); \n",
    "axes[1].set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Patterns in a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(20,4), dpi=100)\n",
    "pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/guinearice.csv', parse_dates=['date'], index_col='date').plot(title='Trend Only', legend=False, ax=axes[0])\n",
    "\n",
    "pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/sunspotarea.csv', parse_dates=['date'], index_col='date').plot(title='Seasonality Only', legend=False, ax=axes[1])\n",
    "\n",
    "pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/AirPassengers.csv', parse_dates=['date'], index_col='date').plot(title='Trend and Seasonality', legend=False, ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Additive and multiplicative time series\n",
    "\n",
    "Additive time series:\n",
    "\n",
    "Value = Base Level + Trend + Seasonality + Error\n",
    "\n",
    "Multiplicative Time Series:\n",
    "\n",
    "Value = Base Level x Trend x Seasonality x Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How to decompose a time series into its components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Import Data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(df['value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(df['value'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\n",
    "result_add.plot().suptitle('Additive Decompose', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Stationary and non-stationary Time Series\n",
    "\n",
    "Stationarity is a property of a time series. A stationary series is one where the values of the series is not a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. How to make a Time Series stationary?\n",
    "\n",
    "You can make series stationary by:\n",
    "\n",
    "- Differencing the Series (once or more)\n",
    "- Take the log of the series\n",
    "- Take the nth root of the series\n",
    "- Combination of the above\n",
    "\n",
    "Forecasting a stationary series is relatively easy and the forecasts are more reliable.\n",
    "\n",
    "We know that linear regression works best if the predictors (X variables) are not correlated against each other. \n",
    "\n",
    "Stationarizing the series removes any persistent autocorrelation, thereby making the predictors(lags of the series) in the forecasting models nearly independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. How to test for stationarity?\n",
    "\n",
    "There are multiple implementations of Unit Root tests like:\n",
    "\n",
    "- Augmented Dickey Fuller test (ADH Test)\n",
    "- Kwiatkowski-Phillips-Schmidt-Shin – KPSS test (trend stationary)\n",
    "- Philips Perron test (PP Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'])\n",
    "\n",
    "# ADF Test\n",
    "result = adfuller(df.value.values, autolag='AIC')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')\n",
    "\n",
    "# KPSS Test\n",
    "result = kpss(df.value.values, regression='c')\n",
    "print('\\nKPSS Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "for key, value in result[3].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. What is the difference between white noise and a stationary series?\n",
    "\n",
    "In white noise there is no pattern whatsoever. \n",
    "\n",
    "Mathematically, a sequence of completely random numbers with mean zero is a white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randvals = np.random.randn(1000)\n",
    "pd.Series(randvals).plot(title='Random White Noise', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. How to detrend a Time Series?\n",
    "\n",
    "Detrending a time series is to remove the trend component from a time series. \n",
    "\n",
    "There are multiple approaches:\n",
    "\n",
    "- Subtract the line of best fit from the time series. The line of best fit may be obtained from a linear regression model with the time steps as the predictor. For more complex trends, you may want to use quadratic terms (x^2) in the model.\n",
    "- Subtract the trend component obtained from time series decomposition we saw earlier.\n",
    "- Subtract the mean.\n",
    "- Apply a filter like Baxter-King filter(statsmodels.tsa.filters.bkfilter) or the Hodrick-Prescott Filter (statsmodels.tsa.filters.hpfilter) to remove the moving average trend lines or the cyclical components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scipy: Subtract the line of best fit\n",
    "from scipy import signal\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'])\n",
    "detrended = signal.detrend(df.value.values)\n",
    "plt.plot(detrended)\n",
    "plt.title('Drug Sales detrended by subtracting the least squares fit', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using statmodels: Subtracting the Trend Component.\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "result_mul = seasonal_decompose(df['value'], model='multiplicative', extrapolate_trend='freq')\n",
    "detrended = df.value.values - result_mul.trend\n",
    "plt.plot(detrended)\n",
    "plt.title('Drug Sales detrended by subtracting the trend component', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. How to deseasonalize a Time Series?\n",
    "\n",
    "There are multiple approaches to deseasonalize a time series as well:\n",
    "\n",
    "- Take a moving average with length as the seasonal window. This will smoothen in series in the process.\n",
    "\n",
    "- Seasonal difference the series (subtract the value of previous season from the current value).\n",
    "\n",
    "- Divide the series by the seasonal index obtained from STL decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting the Trend Component.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Time Series Decomposition\n",
    "result_mul = seasonal_decompose(df['value'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Deseasonalize\n",
    "deseasonalized = df.value.values / result_mul.seasonal\n",
    "\n",
    "# Plot\n",
    "plt.plot(deseasonalized)\n",
    "plt.title('Drug Sales Deseasonalized', fontsize=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. How to test for seasonality of a Time Series?\n",
    "\n",
    "The types of seasonality is determined by the clock or the calendar:\n",
    "\n",
    "- Hour of day\n",
    "- Day of month\n",
    "- Weekly\n",
    "- Monthly\n",
    "- Yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv')\n",
    "\n",
    "# Draw Plot\n",
    "plt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})\n",
    "autocorrelation_plot(df.value.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. How to treat missing values in a Time Series?\n",
    "\n",
    "Some effective alternatives to imputation are:\n",
    "\n",
    "- Backward Fill\n",
    "- Linear Interpolation\n",
    "- Quadratic interpolation\n",
    "- Mean of nearest neighbors\n",
    "- Mean of seasonal couterparts\n",
    "\n",
    "General 'rules of thumb':\n",
    "\n",
    "- If you have explanatory variables use a prediction model like the random forest or k-Nearest Neighbors to predict it.\n",
    "- If you have enough past observations, forecast the missing values.\n",
    "- If you have enough future observations, backcast the missing values\n",
    "- Forecast of counterparts from previous cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate dataset\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "df_orig = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'], index_col='date').head(100)\n",
    "df = pd.read_csv('datasets/a10_missings.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "fig, axes = plt.subplots(7, 1, sharex=True, figsize=(10, 12))\n",
    "plt.rcParams.update({'xtick.bottom' : False})\n",
    "\n",
    "## 1. Actual -------------------------------\n",
    "df_orig.plot(title='Actual', ax=axes[0], label='Actual', color='red', style=\".-\")\n",
    "df.plot(title='Actual', ax=axes[0], label='Actual', color='green', style=\".-\")\n",
    "axes[0].legend([\"Missing Data\", \"Available Data\"])\n",
    "\n",
    "## 2. Forward Fill --------------------------\n",
    "df_ffill = df.ffill()\n",
    "error = np.round(mean_squared_error(df_orig['value'], df_ffill['value']), 2)\n",
    "df_ffill['value'].plot(title='Forward Fill (MSE: ' + str(error) +\")\", ax=axes[1], label='Forward Fill', style=\".-\")\n",
    "\n",
    "## 3. Backward Fill -------------------------\n",
    "df_bfill = df.bfill()\n",
    "error = np.round(mean_squared_error(df_orig['value'], df_bfill['value']), 2)\n",
    "df_bfill['value'].plot(title=\"Backward Fill (MSE: \" + str(error) +\")\", ax=axes[2], label='Back Fill', color='firebrick', style=\".-\")\n",
    "\n",
    "## 4. Linear Interpolation ------------------\n",
    "df['rownum'] = np.arange(df.shape[0])\n",
    "df_nona = df.dropna(subset = ['value'])\n",
    "f = interp1d(df_nona['rownum'], df_nona['value'])\n",
    "df['linear_fill'] = f(df['rownum'])\n",
    "error = np.round(mean_squared_error(df_orig['value'], df['linear_fill']), 2)\n",
    "df['linear_fill'].plot(title=\"Linear Fill (MSE: \" + str(error) +\")\", ax=axes[3], label='Cubic Fill', color='brown', style=\".-\")\n",
    "\n",
    "## 5. Cubic Interpolation --------------------\n",
    "f2 = interp1d(df_nona['rownum'], df_nona['value'], kind='cubic')\n",
    "df['cubic_fill'] = f2(df['rownum'])\n",
    "error = np.round(mean_squared_error(df_orig['value'], df['cubic_fill']), 2)\n",
    "df['cubic_fill'].plot(title=\"Cubic Fill (MSE: \" + str(error) +\")\", ax=axes[4], label='Cubic Fill', color='red', style=\".-\")\n",
    "\n",
    "# Interpolation References:\n",
    "# https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html\n",
    "# https://docs.scipy.org/doc/scipy/reference/interpolate.html\n",
    "\n",
    "## 6. Mean of 'n' Nearest Past Neighbors ------\n",
    "def knn_mean(ts, n):\n",
    "    out = np.copy(ts)\n",
    "    for i, val in enumerate(ts):\n",
    "        if np.isnan(val):\n",
    "            n_by_2 = np.ceil(n/2)\n",
    "            lower = np.max([0, int(i-n_by_2)])\n",
    "            upper = np.min([len(ts)+1, int(i+n_by_2)])\n",
    "            ts_near = np.concatenate([ts[lower:i], ts[i:upper]])\n",
    "            out[i] = np.nanmean(ts_near)\n",
    "    return out\n",
    "\n",
    "df['knn_mean'] = knn_mean(df.value.values, 8)\n",
    "error = np.round(mean_squared_error(df_orig['value'], df['knn_mean']), 2)\n",
    "df['knn_mean'].plot(title=\"KNN Mean (MSE: \" + str(error) +\")\", ax=axes[5], label='KNN Mean', color='tomato', alpha=0.5, style=\".-\")\n",
    "\n",
    "## 7. Seasonal Mean ----------------------------\n",
    "def seasonal_mean(ts, n, lr=0.7):\n",
    "    \"\"\"\n",
    "    Compute the mean of corresponding seasonal periods\n",
    "    ts: 1D array-like of the time series\n",
    "    n: Seasonal window length of the time series\n",
    "    \"\"\"\n",
    "    out = np.copy(ts)\n",
    "    for i, val in enumerate(ts):\n",
    "        if np.isnan(val):\n",
    "            ts_seas = ts[i-1::-n]  # previous seasons only\n",
    "            if np.isnan(np.nanmean(ts_seas)):\n",
    "                ts_seas = np.concatenate([ts[i-1::-n], ts[i::n]])  # previous and forward\n",
    "            out[i] = np.nanmean(ts_seas) * lr\n",
    "    return out\n",
    "\n",
    "df['seasonal_mean'] = seasonal_mean(df.value, n=12, lr=1.25)\n",
    "error = np.round(mean_squared_error(df_orig['value'], df['seasonal_mean']), 2)\n",
    "df['seasonal_mean'].plot(title=\"Seasonal Mean (MSE: \" + str(error) +\")\", ax=axes[6], label='Seasonal Mean', color='blue', alpha=0.5, style=\".-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. What are autocorrelation and partial autocorrelation functions?\n",
    "\n",
    "Autocorrelation is simply the correlation of a series with its own lags. \n",
    "\n",
    "Partial Autocorrelation is the pure correlation of a series and its lag, excluding the correlation contributions from the intermediate lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv')\n",
    "\n",
    "# Calculate ACF and PACF upto 50 lags\n",
    "# acf_50 = acf(df.value, nlags=50)\n",
    "# pacf_50 = pacf(df.value, nlags=50)\n",
    "\n",
    "# Draw Plot\n",
    "fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\n",
    "plot_acf(df.value.tolist(), lags=50, ax=axes[0])\n",
    "plot_pacf(df.value.tolist(), lags=50, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. How to compute partial autocorrelation function?\n",
    "\n",
    "The partial autocorrelation of lag (k) of a series is the coefficient of that lag in the autoregression equation of Y. \n",
    "\n",
    "The autoregressive equation of Y is nothing but the linear regression of Y with its own lags as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Lag Plots\n",
    "\n",
    "A Lag plot is a scatter plot of a time series against a lag of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n",
    "\n",
    "# Import\n",
    "ss = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/sunspotarea.csv')\n",
    "a10 = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv')\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\n",
    "for i, ax in enumerate(axes.flatten()[:4]):\n",
    "    lag_plot(ss.value, lag=i+1, ax=ax, c='firebrick')\n",
    "    ax.set_title('Lag ' + str(i+1))\n",
    "\n",
    "fig.suptitle('Lag Plots of Sun Spots Area \\n(Points get wide and scattered with increasing lag -> lesser correlation)\\n', y=1.15)    \n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\n",
    "for i, ax in enumerate(axes.flatten()[:4]):\n",
    "    lag_plot(a10.value, lag=i+1, ax=ax, c='firebrick')\n",
    "    ax.set_title('Lag ' + str(i+1))\n",
    "\n",
    "fig.suptitle('Lag Plots of Drug Sales', y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. How to estimate the forecastability of a Time Series?\n",
    "\n",
    "The ‘Approximate Entropy’ can be used to quantify the regularity and unpredictability of fluctuations in a time series.\n",
    "\n",
    "Sample Entropy is similar to approximate entropy but is more consistent in estimating the complexity even for smaller time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate Entropy:\n",
    "# https://en.wikipedia.org/wiki/Approximate_entropy\n",
    "ss = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/sunspotarea.csv')\n",
    "a10 = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv')\n",
    "rand_small = np.random.randint(0, 100, size=36)\n",
    "rand_big = np.random.randint(0, 100, size=136)\n",
    "\n",
    "def ApEn(U, m, r):\n",
    "    \"\"\"Compute Aproximate entropy\"\"\"\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "\n",
    "    def _phi(m):\n",
    "        x = [[U[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = [len([1 for x_j in x if _maxdist(x_i, x_j) <= r]) / (N - m + 1.0) for x_i in x]\n",
    "        return (N - m + 1.0)**(-1) * sum(np.log(C))\n",
    "\n",
    "    N = len(U)\n",
    "    return abs(_phi(m+1) - _phi(m))\n",
    "\n",
    "print(ApEn(ss.value, m=2, r=0.2*np.std(ss.value)))     # 0.651\n",
    "print(ApEn(a10.value, m=2, r=0.2*np.std(a10.value)))   # 0.537\n",
    "print(ApEn(rand_small, m=2, r=0.2*np.std(rand_small))) # 0.143\n",
    "print(ApEn(rand_big, m=2, r=0.2*np.std(rand_big))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Entropy:\n",
    "# https://en.wikipedia.org/wiki/Sample_entropy\n",
    "def SampEn(U, m, r):\n",
    "    \"\"\"Compute Sample entropy\"\"\"\n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "\n",
    "    def _phi(m):\n",
    "        x = [[U[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = [len([1 for j in range(len(x)) if i != j and _maxdist(x[i], x[j]) <= r]) for i in range(len(x))]\n",
    "        return sum(C)\n",
    "\n",
    "    N = len(U)\n",
    "    return -np.log(_phi(m+1) / _phi(m))\n",
    "\n",
    "print(SampEn(ss.value, m=2, r=0.2*np.std(ss.value)))      # 0.78\n",
    "print(SampEn(a10.value, m=2, r=0.2*np.std(a10.value)))    # 0.41\n",
    "print(SampEn(rand_small, m=2, r=0.2*np.std(rand_small)))  # 1.79\n",
    "print(SampEn(rand_big, m=2, r=0.2*np.std(rand_big)))      # 2.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Why and How to smoothen a Time Series?\n",
    "\n",
    "Smoothening of a time series may be useful in:\n",
    "\n",
    "- Reducing the effect of noise in a signal get a fair approximation of the noise-filtered series.\n",
    "- The smoothed version of series can be used as a feature to explain the original series itself.\n",
    "- Visualize the underlying trend better\n",
    "\n",
    "How to smoothen a series? \n",
    "\n",
    "- Take a moving average.\n",
    "- Do a LOESS smoothing (Localized Regression).\n",
    "- Do a LOWESS smoothing (Locally Weighted Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. How to use Granger Causality test to know if one Time Series is helpful in forecasting another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "plt.rcParams.update({'xtick.bottom' : False, 'axes.titlepad':5})\n",
    "\n",
    "# Import\n",
    "df_orig = pd.read_csv('datasets/elecequip.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# 1. Moving Average\n",
    "df_ma = df_orig.value.rolling(3, center=True, closed='both').mean()\n",
    "\n",
    "# 2. Loess Smoothing (5% and 15%)\n",
    "df_loess_5 = pd.DataFrame(lowess(df_orig.value, np.arange(len(df_orig.value)), frac=0.05)[:, 1], index=df_orig.index, columns=['value'])\n",
    "df_loess_15 = pd.DataFrame(lowess(df_orig.value, np.arange(len(df_orig.value)), frac=0.15)[:, 1], index=df_orig.index, columns=['value'])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4,1, figsize=(7, 7), sharex=True, dpi=120)\n",
    "df_orig['value'].plot(ax=axes[0], color='k', title='Original Series')\n",
    "df_loess_5['value'].plot(ax=axes[1], title='Loess Smoothed 5%')\n",
    "df_loess_15['value'].plot(ax=axes[2], title='Loess Smoothed 15%')\n",
    "df_ma.plot(ax=axes[3], title='Moving Average (3)')\n",
    "fig.suptitle('How to Smoothen a Time Series', y=0.95, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use Granger Causality test to know if one time series is helpful in forecasting another?\n",
    "\n",
    "Granger causality is based on the idea that if X causes Y, then the forecast of Y based on previous values of Y AND the previous values of X should outperform the forecast of Y based on previous values of Y alone:\n",
    "\n",
    "- It is nicely implemented in the statsmodel package.\n",
    "\n",
    "- It accepts a 2D array with 2 columns as the main argument. The values are in the first column and the predictor (X) is in the second column.\n",
    "\n",
    "- The Null hypothesis is: the series in the second column, does not Granger cause the series in the first. \n",
    "\n",
    "- If the P-Values are less than a significance level (0.05) then you reject the null hypothesis and conclude that the said lag of X is indeed useful.\n",
    "\n",
    "- The second argument maxlag says till how many lags of Y should be included in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse_dates=['date'])\n",
    "df['month'] = df.date.dt.month\n",
    "grangercausalitytests(df[['value', 'month']], maxlag=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
